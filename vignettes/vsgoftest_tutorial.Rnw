% \VignetteIndexEntry{Tutorial}
% \VignetteDepends{vsgoftest}
% \VignetteKeyword{vsgoftest}
% \VignetteKeyword{tutorial}
% \VignetteKeyword{manual}
% \VignetteEngine{knitr::knitr}


\documentclass[a4paper, 10pt]{article}

\usepackage[a4paper, total={470pt, 583pt}]{geometry}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}



\def\firstMarker{$^*$}
\def\secondMarker{$\dagger$}

\def\pkg{\textbf}
\def\proglang{\textsf}

\newcommand{\der}{\mbox{d}}

\newcommand{\KK}{\mathbb{K}}
\newcommand{\HHH}{\mathbb{H}}
\newcommand{\SSS}{\mathbb{S}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\mand}{\quad\mbox{and}\quad}
\def\id{\hbox{{\rm\bf 1}\kern-.4em\hbox{{\rm\bf 1}}}\! } %Fonction indicatrice


\newcommand{\kpq}{k}




\author{Justine Lequesne\firstMarker and Philippe Regnault\secondMarker}
\title{Package \pkg{vsgoftest} for \proglang{R}: goodness-of-fit tests based on Kullback-Leibler divergence}

\begin{document}

\maketitle
\begin{center}
\firstMarker Service de Recherche Clinique, Centre Henri Becquerel,  76038 Rouen Cedex1, France. E-mail: justine.lequesne@chb.unicancer.fr \\
\secondMarker 
Laboratoire de Math{\'e}matiques de Reims, FRE 2011, Universit√© de Reims Champagne-Ardenne, BP 1039, 51687 Reims cedex 2, France. E-mail: {philippe.regnault@univ-reims.fr}
\end{center}



The package \pkg{vsgoftest} provides functions to estimate Shannon entropy of continuous random variables and to test the goodness-of-fit of a vector of real numbers to some prescribed family of distributions. As a by-product, it also provides functions to compute the density, cumulative density and quantile functions of Pareto and Laplace distributions, as well as to generate samples from Pareto and Laplace distributions. 

The latest (under development) version of the package \pkg{vsgoftest} is available and can be installed in \proglang{R} from the github repository of the project as follows:

<<echo = FALSE>>=
knitr::opts_chunk$set(comment = '')
@


<<echo = TRUE, eval = FALSE>>=
#Package devtools must be installed
devtools::install_github('vsgoftest', dependencies = TRUE)
@

The package is structured around two functions, \verb+entropy.estimate+ and \verb+vs.test+. While the first one aims at computing Vasicek estimator
of the differential entropy $\SSS(P) = -\int p(x) \log p(x) \der x$ of a distribution $P$ on $\R$ with density $p$ from a numeric sample $X_1, \dots, X_n$ drawn from $P$, the second one performs Vasicek-Song Goodness-of-fit test to usual parametric families of distributions. A comprehensive presentation of their usage is proposed in Sections~\ref{MEGOFPSecEntEst} and~\ref{MEGOFPSecVSTest}, with numerous examples. An application to environmental data is presented in Section~\ref{TutoAppliData}.

Details about entropy estimation, Vasicek-Song goodness-of-fit tests and the contents and features of the package are available in the listed references at the end of the present document. Particularly, see~\cite{GirardinLequesne} and \cite{LequesneRegnault}.


\section{Function entropy.estimate for estimating differential entropy} \label{MEGOFPSecEntEst}


The  function \verb+entropy.estimate+ computes the spacing based Vasicek estimator
\begin{equation} \label{MEGOFPEqnVasiEst}
 V_{mn} :=\frac{1}{n} \sum_{i=1}^{n} \log \left( \frac{n}{2m} \left[ X_{(i+m)}-X_{(i-m)} \right] \right)
 %\label{vasicek}
\end{equation}
of the differential Shannon entropy of a numeric sample $X_1, \dots, X_n$, with $X_{(1)} \leq \dots \leq X_{(n)}$ denoting the order statistics of the sample and $m\in\N^*$ being a window size smaller than $n/2$. Two arguments must be provided:
\begin{itemize}
 \item \verb+x+: the numeric sample;
 \item \verb+window+: an integer between 1 and half of the sample size, specifying the window size of the spacing-based estimator~(\ref{MEGOFPEqnVasiEst}).
\end{itemize}
It returns a single value which is the estimate of Shannon differential entropy of the sample. Right below is an example for a sample drawn from a normal distribution with parameters $\mu = 0$ and $\sigma^2 = 1$.


<<>>=
library('vsgoftest')
set.seed(2)	#set seed of PRNG
samp <- rnorm(n = 100, mean = 0, sd = 1) #sampling from normal distribution
entropy.estimate(x = samp, window = 8) #estimating entropy with window = 8
log(2*pi*exp(1))/2 #the true value of entropy
@

One may wonder about the choice of the window size. Following the maximum entropy paradigm, one may choose the window size that maximizes the entropy estimate, as described by the following instructions.

<<>>=
n <- 100 #sample size
V <- numeric(n/2 -1)
for (i in 1:(n/2 -1)) { #make vary window size from 1 to 49
  #compute estimate for each window
  V[i] <- entropy.estimate(x = samp, window = i)
}
which.max(V) #Choose window that maximizes entropy
@


Consider as a second example, a sample drawn from a Pareto distribution whose density is 
$$p(x;c,\mu) = \frac{\mu c^\mu}{x^{\mu+1}}, \quad  x \geq c,$$
where $c >0 $ and $\mu >0 $. The closed form expression of its Shannon entropy is 
$$\SSS(p( . ;c, \mu)) = -\ln \mu + \ln c + \frac{1}{\mu} +1.$$
Such a sample can be obtained by making use of the function \verb+rpareto+, as illustrated by the following instructions. 

<<>>=
set.seed(5)
n <- 100 #Sample size
samp <- rpareto(n, c = 1, mu = 2) #sampling from Pareto distribution
entropy.estimate(x = samp, window = 3)
-log(2) + 3/2 #True value of entropy
@



\section{Function vs.test for testing GOF to a specified model} \label{MEGOFPSecVSTest}


The function \verb+vs.test+ performs the Vasicek-Song procedure for testing goodness-of-fit of a numerical sample to whether a prescribed distribution $P ={P}_{0}(\theta)$, the so-called simple null hypothesis test
\begin{equation}\label{arthyps}
H_{0}: P ={P}_{0}(\theta) \quad\mbox{against}\quad
H_{1}: P \ne {P}_{0}(\theta),
\end{equation}
or to a parametric family $\cP_{0}(\Theta)$, the so-called composite null hypothesis test
\begin{equation}\label{arthyp}
H_{0}: P \in \cP_{0}(\Theta) \quad\mbox{against}\quad
H_{1}: P \notin \cP_{0}(\Theta);
\end{equation}
see Appendix below for details. In its shortest call, it requires two arguments to be provided:
\begin{itemize}
 \item \verb+x+: the numeric sample;
 \item \verb+densfun+: a character string specifying the targeted family of distributions.  Available families of distributions are: uniform, normal, log-normal, exponential, gamma, Weibull, Pareto, Fisher and Laplace distributions. They are referred to by the symbolic name in \proglang{R} of their density function. For example, set \verb+densfun = 'dnorm'+ to test GOF to the family of normal distributions.
\end{itemize}

It returns an object of class \verb+htest+, i.e., a list whose main components are:
\begin{itemize}
 \item \verb+statistic+: the value of the VS test statistic~(\ref{PKGEqnTestStat}) of the sample, for the optimal window size, as defined in~(\ref{arteqm});
 \item \verb+parameter+: the optimal window size;
 \item \verb+estimate+: the maximum likelihood estimate of the parameters of the distribution to which the GOF is tested;
 \item \verb+p.value+: the p-value associated to the sample.
\end{itemize}

By default, once provided the arguments \verb+x+ and \verb+densfun+, the function \verb+vs.test+ performs the composite GOF VS test to the prescribed family \verb+densfun+. Depending on the sample size, the p-value is estimated by means of Monte-Carlo methods (if the sample size is smaller than 80), or through the asymptotic distribution~(\ref{asymptoticn}) of the VS test statistic.


In the following example, a normally distributed sample is simulated. Its goodness-of-fit to the family of Laplace distributions is rejected while its normality is accepted.

<<>>=
set.seed(2)
samp <- rnorm(50,2,3)
vs.test(x = samp, densfun = 'dlaplace')
@

An additional argument can be provided to perform a simple null hypothesis test. By setting \verb+param+ to a suitable numeric vector (adapted to the family of distribution given in \verb+densfun+), the function~\verb+vs.test+ performs a GOF test to the single prescribed distribution.

<<>>=
set.seed(26)
vs.test(x = samp, densfun = 'dnorm', param = c(2,3))
@

Note that when \verb+param+ is provided, the MLE of the parameter(s) of the null distribution is not computed, hence the component \verb+estimate+ of the result is not available.

One may prefer estimating the p-value of the sample by Monte-Carlo simulations, even when sample size is larger than 80. The optional argument \verb+simulate.p.value+ has then to be turned to \verb+TRUE+ (\verb+NULL+ by default). Note that, it is also possible to choose the number of Monte-Carlo replicates by providing to the optional argument \verb+B+ a positive integer (default is \verb+B = 5000+).

<<>>=
set.seed(1)
samp <- rweibull(200, shape = 1.05, scale = 1)
vs.test(samp, densfun = 'dexp')
@

<<>>=
set.seed(2)
vs.test(samp, densfun = 'dexp', simulate.p.value = TRUE, B = 10000)
@

Whether \verb+simulate.p.value+ is turned to \verb+TRUE+ or not, Vasicek estimates $V_{mn}$ are computed for all $m$ from $1$ to $n^{1/3-\delta}$, where $\delta < 1/3$; hence the test statistic is $I_{\widehat{m}n}$, given by~(\ref{PKGEqnTestStat}) for $\widehat{m}$ the optimal window size, as defined in~(\ref{arteqm}).
The choice of $\delta$ depends on the family the goodness-of-fit is tested to. Precisely, for Weibull, Pareto, Fisher, Laplace and Beta families, $\delta$ is set to $2/15$ while for uniform, normal, log-normal, exponential and gamma families, it is set to $1/12$. These default settings result from numerous experimentations. Still, if needed, the user can tune this parameter by providing a numeric value to the optional argument \verb+delta+.

<<>>=
vs.test(samp, densfun = 'dexp', delta = 5/30)
@

In addition, when estimating the p-value by means of Monte-Carlo simulations, upper-bounding the window size by $n^{1/3-\delta}$ is not necessary (it is a necessary requirement only for using the asymptotic normality of $I_{mn}$ and hence for computing asymptotic p-values from~(\ref{asymptoticn})). This upper-bound can be relaxed so that $m$ ranges from $1$ to $n/2$ by adding \verb+extend = TRUE+. The interests are multiple. First, enlarging the range for $m$ may lead to a most reliable test, as illustrated below.

<<>>=
set.seed(8)
samp <- rexp(30, rate = 3)
vs.test(x = samp, densfun = "dlnorm")
@

<<>>=
vs.test(x = samp, densfun = "dlnorm", extend = TRUE)
@

Second, enlarging the range for $m$  is also interesting when the sample size is moderate, especially if ties are present. Indeed, the presence of ties is particularly non-appropriate for performing VS tests: if ties are present, it may happen that some spacings $X_{(i+m)} - X_{(i-m)}$ vanish and hence Vasicek estimate is not well-defined. From a computational view point, it requires the window size $m$ to be greater than the maximal number of ties in the sample. Hence, if the upper-bound $n^{1/3-\delta}$ is less than the maximal number of ties, the test statistic can not be computed. 
Turning \verb+extend+ to \verb+TRUE+ may avoid this behaviour, as illustrated below.

<<echo = TRUE, eval = TRUE, error = TRUE>>=
samp <- c(samp, rep(4,3)) #add ties in the previous sample
vs.test(x = samp, densfun = "dexp")
@

<<>>=
vs.test(x = samp, densfun = "dexp", extend = TRUE)
@

Finally, it may happen that for all $m$ between $1$ and $n^{1/3-\delta}$, Vasicek's estimate $V_{mn}$ exceeds the parametric estimate of the entropy of the null distribution, hence no window size exists satisfying~(\ref{arteqm}), as illustrated below.

<<echo = TRUE, eval = FALSE>>=
set.seed(84)
ech <- rpareto(20, mu = 1/2, c = 1)
vs.test(x = ech, densfun = 'dpareto', param = c(1/2, 1))
@

<<echo = FALSE, eval = TRUE>>=
set.seed(84)
ech <- rpareto(20, mu = 1/2, c = 1)
try(vs.test(x = ech, densfun = 'dpareto', param = c(1/2, 1)))
@

By turning \verb+extend+ to \verb+TRUE+, the possible values for the window size is enlarged, possibly enabling the existence of Vasicek estimates smaller than empirical entropy. 
% The following example illustrates this behaviour.


Note that when estimating the p-value by Monte-Carlo methods, it may happen that for some replicates, the constraint~(\ref{PKGEqnConstraint}) is not satisfied whatever be the window size. These replicates are then ignored and the p-value is computed from remaining replicates. A warning message is added to the output, informing the user on the number of ignored replicates.

<<>>=
data(contaminants) #load data from package vsgoftest; see ?contaminants
set.seed(1)
vs.test(x = aluminium2, densfun = 'dpareto')
@

A large proportion of such ignored replicates may indicate that the original sample is too small or obviously does not fit the null distribution.

\section{Application to real data} \label{TutoAppliData}

This Section is devoted to the application to real data of Vasicek-Song GOF tests. The package~\pkg{vs.test} provides environmental data originating from a guidance report edited by the Technology Support Center of the United States Environmental Protection Agency; see~\cite{singh}. According to~\cite{singh}, environmental scientists may have to take remediation decisions at suspected sites based on organic and inorganic contaminant concentration measurements. From a statistical point of view, these decisions are usually derived from the computation of confidence upper bounds for contaminant concentrations. Testing the goodness-of-fit of the distribution of data to specified models hence appears of prior importance. \cite{singh} also point out that contaminant concentration data from sites quite often appear to follow a skewed probability distribution, making the log-normal family a frequently-used model. They illustrate their purpose by applying Shapiro-Wilk test to the log-transformed of the samples \verb+aluminium1+, \verb+manganese+, \verb+aluminium2+ and \verb+toluene+ (stored into the present package)\footnote{A succinct description of these data is available by evaluating the following \proglang{R} command: ?contaminants}.

The following code chunks intend to illustrate the applicability and behaviour of the function \verb+vs.test+ on these environmental data. The significant level is fixed to $0.1$ as in \cite{singh}. Note that, because the four samples are small (at most 23 observations), all p-values presented below are estimated by means of Monte-Carlo methods. Note also that warning messages notifying that there are ties in the samples have been dropped out from outputs.

<<warning = FALSE>>=
set.seed(1)
vs.test(x = aluminium1, densfun = 'dlnorm')
@

The log-normality hypothesis is accepted for \verb+aluminium1+. Similar results are obtained for \verb+manganese+. Log-normality is rejected for \verb+aluminium2+.

<<warning = FALSE>>=
set.seed(1)
vs.test(x = aluminium2, densfun = 'dlnorm')
@

Due to numerous ties in \verb+toluene+, \verb+vs.test+ can not compute Vasicek entropy estimate unless \verb+extend+ is turned to \verb+TRUE+. Still, \verb+vs.test+ notifies that the constraint~(\ref{PKGEqnConstraint}) is violated for all window sizes, which suggests that data are not likely to be drawn from log-normal distribution. Turning \verb+relax+ to \verb+TRUE+ yields the following result.

<<warning = FALSE>>=
set.seed(1)
vs.test(x = toluene, densfun = 'dlnorm', extend = TRUE, relax = TRUE)
@

Still, this last result looks spurious as the test statistic is negative (resulting from the constraint~(\ref{PKGEqnConstraint}) to be disabled by \verb+relax = TRUE+). Alternatively, it is possible to test normality of the log-transformed sample as follows.

<<warning = FALSE>>=
set.seed(1)
vs.test(x = log(toluene), densfun ='dnorm', extend = TRUE)
@

In summary, log-normality is accepted for \verb+aluminium1+ and \verb+manganese+ while it is rejected for \verb+aluminium2+ and \verb+toluene+. These results are consistent with those obtained by \cite{singh}. To go a step further, the goodness-of-fit to Pareto distribution family can also be performed for \verb+aluminium2+ and \verb+toluene+. Log-normal and  Pareto distributions have historically competed with sometimes closely related generating processes and hard-to-distinguish tail properties. Goodness-of-fit of \verb+aluminium2+ to Pareto distribution family is rejected.

<<warning = FALSE>>=
set.seed(1)
vs.test(x = aluminium2, densfun = 'dpareto')
@

Applying \verb+vs.test+ to \verb+toluene+ with default settings does not yield any result because of numerous ties and the constraint~(\ref{PKGEqnConstraint}) being violated.
Again, turning \verb+extend+ and \verb+relax+ to \verb+TRUE+ yields the following spurious result.

<<warning = FALSE>>=
set.seed(12)
vs.test(x = toluene, densfun = 'dpareto', extend = TRUE, relax = TRUE)
@

Finally, it is possible to test uniformity of the sample transformed by the cumulative density function of the Pareto distribution, as follows, yielding to accept the goodness-of-fit of \verb+toluene+ to the Pareto distribution family.

<<warning = FALSE>>=
#First, compute the MLE of parameters of Pareto dist.
res.test <- vs.test(x = toluene,
                    densfun = 'dpareto',
                    extend = TRUE, relax = TRUE)
#Then, test uniformity of transformed data
set.seed(5)
vs.test(x = ppareto(toluene,
                    mu = res.test$estimate[1],
                    c = res.test$estimate[2]),
        densfun ='dunif', param = c(0,1), extend = TRUE)
@

\section*{Appendix: Vasicek-Song tests, theoretical background}

\cite{song} proposes a goodness-of-fit test based on Kullback-Leibler divergence for either simple (\ref{arthyps}) or composite (\ref{arthyp}) null hypotheses. Precisely, the test statistic $I_{mn}$ is an estimator of the Kullback-Leibler divergence $\KK(P|P_0(\theta)) = -\SSS(P) - \int p_0(x;\theta) p(x) \der x$ of the sampled distribution $P$, with respect to the null distribution $P_0(\theta)$ (with respective densities $p$ and $p_0(.;\theta)$) in case of a simple hypothesis or some estimate $P_0(\widehat{\theta}_n)$ otherwise:
\begin{equation} \label{artqimn}
 I_{mn} := -V_{mn}-\frac{1}{n} \sum_{i=1}^{n} \log p_{0}(X_{i},{\widehat{\theta}}_{n}),
\end{equation}
where $V_{mn}$, given by~(\ref{MEGOFPEqnVasiEst}), estimates $\SSS(P)$ while $- \frac{1}{n}\sum_{i=1}^{n} \log p_{0}(X_{i},{\widehat{\theta}}_{n})$ estimates $-\int_{\R} \log p_0(x;\theta) p(x) \der x$. For the test~(\ref{arthyps}) with simple null hypothesis, set $\widehat{\theta}_n = \theta$, where $\theta$ is the null parameter. 
Otherwise, $\widehat{\theta}_n$ is the maximum likelihood estimator (MLE) of $\theta$, i.e., it satisfies 
$$\frac{1}{n}\sum_{i=1}^n \log p_{0}(X_{i},{\widehat{\theta}}_{n}) = \max_{\theta \in \Theta} \frac{1}{n} \sum_{i=1}^{n} \log p_{0}(X_{i},\theta).$$

\cite{song} establishes the asymptotic behaviour of $I_{mn}$, independently of the null hypothesis. Precisely, $I_{mn}$ is consistent and asymptotically normally distributed, provided the distribution of the sample belongs to the following class of distributions: 
\begin{equation}
\cF=\left\{ P\in\mathcal{D}: \sup_{x:\; 0<F(x)<1} \frac{\vert p'(x) \vert}{p^{2}(x)} F(x)[1-F(x)] < \gamma \right\},
 \label{artfamilleasymptotique}
 \end{equation}
for some $\gamma>0$, where $F$  is the cumulative density function of $P$, with density $p$ whose derivative is  ${p'}$ (almost every where).
The class $\cF$ contains the most classical distributions such as uniform ($\gamma=0$), normal, exponential and gamma ($\gamma=1$), Fisher ($\gamma=(2+\nu_2)/\nu_2$ where $\nu_2$ is the second degree of freedom), Pareto ($\gamma=(\mu+1)/\mu$ where $\mu$ is the shape parameter), etc. 
If $\cP_0(\Theta)\subset\cF$, and if
\begin{equation} \label{PKGEqnConditionsWindow}
m/\log n \xrightarrow[n \rightarrow \infty]{} 0 \quad \textrm{and} \quad m(\log n)^{2/3}/n^{1/3} \xrightarrow[n \rightarrow \infty]{} 0,
\end{equation}
then
\begin{equation} \label{asymptoticn}
\sqrt{6mn}[I_{mn}-\log(2m)+\psi(2m)] \xrightarrow{\cal D} \mathcal{N}(0,1),
\end{equation}
where $\psi(m)$ is the digamma function.
The asymptotic bias  $\log (2m)-\psi(2m)$ of $I_{mn}$ is that  of $-V_{mn}$. \cite{song} points out that $I_{mn}$ may have an additional substantial bias for small samples and suggests the following bias correction in the asymptotic distribution~(\ref{asymptoticn}), from which decision rule can be consistently derived for moderate and large sample sizes:
\begin{equation} \label{asymptoticn2}
\sqrt{6mn}\left[ I_{mn}-b_{mn} \right] \xrightarrow{\cal D} \mathcal{N}(0,1),
\end{equation}
where 
$$b_{mn} = \log(2m) - \log(n) -\psi(2m) + \psi(n+1) +\frac{2m}{n} R_{2m-1} - \frac{2}{n} \sum_{i=1}^m R_{i+m-2},$$
with $R_{m} = \sum_{j=1}^m 1/j$.
Through (\ref{asymptoticn2}), an asymptotic p-value for the related VS test is derived, given by
\begin{equation} \label{asymppvalue}
p=1-\Phi^{-1}\left( \sqrt{6mn}\left[I_{mn}(x_1^n)-b_{mn}\right]\right),
\end{equation}
where $I_{mn} (x_1^n)$ denotes the value of the statistic $I_{mn}$ for the observations $x_1^n = (x_1, \dots, x_n)$ and $\Phi$ denotes the cumulative density function of the normal distribution. According to~\cite{song}, the asymptotic p-value (\ref{asymppvalue}) provides accurate results when the sample size $n$ is at least 80.

For small sample sizes, Monte Carlo simulations may be preferred for computing p-values, as follows. 
A large number $N$ of replications of the sample $X_1^n$  drawn from the distribution $P_{0}(\widehat{\theta}_{n})$ (or $P_{0}(\theta)$ in case of simple null hypothesis) are generated.
The test statistic $I_{mn}^{i}$ is computed for each replication $i, 1\leq i\leq N.$ 
The p-value is then given by the empirical mean $(\sum_{i=1}^{N}\id_{\{I_{mn}^{i}>I_{mn}(x_1^n)\}})/N.$


For choosing $m$, \cite{song} proposes to minimize $I_{mn}$  -- that is maximize $V_{mn}$, with respect to~$m$, yielding the most conservative test. The author notes also that the values of $m$ for which $I_{mn}$ is negative have to be excluded. Indeed, such negative values for $I_{mn}$ constitute poor estimates of the non-negative divergence $\KK(P |P_0(\theta))$. Hence, $m$ has to be chosen subject to the constraint
\begin{equation} \label{PKGEqnConstraint}
V_{mn} \leq - \frac{1}{n} \sum_{i=1}^n \log (p_{0}(.; \widehat{\theta}_n)).
\end{equation}
Finally, the window size selected by Song -- say the optimal window size, is
\begin{equation}\label{arteqm}
% \widehat{m}=\min\limits_{ 1\leq m^{*} < \lfloor n^{1/3-\delta}\rfloor} \left\{ m^* = \arg\!\!\max_{m\in\N^*} V_{mn} : \; V_{mn}\leq -\frac{1}{n} \sum_{i=1}^{n} \log p_{0}(X_{i}, \widehat{\theta}_{n}) \right\},
\widehat{m}= \min
   \left\{
    m^* \in \arg\!\!\max_{m\in\N^*} 
    \left\{ 
      V_{mn} : \; V_{mn}\leq -\frac{1}{n} \sum_{i=1}^{n} \log p_{0}(X_{i}, \widehat{\theta}_{n})
    \right\}
    : 1 \leq m^{*} < \lfloor n^{1/3-\delta} \rfloor
   \right\},
%\widehat{m}=\min \left\{ m^* = \arg\!\!\max_{m\in\N^*} V_{mn} : V_{mn}\leq -\frac{1}{n} \sum_{i=1}^{n} \log p_{0}(X_{i}, \widehat{\theta}_{n}) \right\}
\end{equation}
for some $\delta \in \R$ such that $1/3-\delta>0$ and the VS test statistic is 
\begin{equation} \label{PKGEqnTestStat}
 I_{\widehat{m}n} = - V_{\widehat{m}n} - \frac{1}{n} \sum_{i=1}^n \log p_0(X_i;\widehat{\theta}_n).
\end{equation}
The upper bound $n^{1/3 - \delta}$ for the window size $m$ is chosen so that conditions~(\ref{PKGEqnConditionsWindow}) are fulfilled and the asymptotic normality~(\ref{asymptoticn}) holds. No systematic optimal choice for $\delta$ exists; it can depend on the family of distributions the GOF is tested to.


\begin{thebibliography}{4}
 
 \bibitem{GirardinLequesne} Girardin V, Lequesne J (2017). \textit{Entropy-based goodness-of-fit test, a unifying framework. Application to DNA replication.} Communications in  Statistics -- Theory and Methods. doi:10.1080/03610926.2017.1401084
 
 \bibitem{LequesneRegnault} Lequesne J, Regnault P (2018) \textit{Package vsgoftest for R: goodness-of-fit tests based on Kullback-Leibler divergence.} URL: 

 \bibitem{singh} Singh AK, Singh A, Engelhardt M (1997). The lognormal distribution in environmental applications. In \textit{Technology Support Center Issue Paper.} Citeseer.
 
 \bibitem{song} Song KS (2002). \textit{Goodness-of-fit tests based on Kullback-Leibler discrimination information.} IEEE Transactions on Information Theory, \textbf{48}(5), 1103-1117.
 

\end{thebibliography}


\end{document}
